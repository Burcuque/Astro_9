{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Linear Regression\n",
    "\n",
    "**Nick Kern**\n",
    "<br>\n",
    "**Astro 9: Python Programming in Astronomy**\n",
    "<br>\n",
    "**UC Berkeley**\n",
    "\n",
    "A very important concept in data analysis (and more broadly statistics) is [linear regression](https://en.wikipedia.org/wiki/Linear_regression). In this lecture, we will explore how to perform linear regression on basic data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we mean by \"linear regression\" is effectively just fitting curves to data. The data may look like it follows some underlying distribution, but it doesn't have to. It could be very noisy data, for example.\n",
    "\n",
    "<img src='imgs/noisy_data.png' width=500px/>\n",
    "<center>Data that follows the same underlying line, but has less noise (left) and more noise (right) along the y-axis</center>\n",
    "\n",
    "Note that the term \"linear regression\" does not mean that the data itself is linear. The data could be linear, but it could also be quadratic, or cubic, etc. We could, for example, use linear regression to fit lines and curves to all of the following datasets.\n",
    "\n",
    "<img src='imgs/lin_reg.png' width=700px/>\n",
    "<center>Three data sets with best-fitting lines using linear regression</center>\n",
    "\n",
    "Linear regression is a way of saying, \"we have some *data*, what is the *model* that can best reproduce the data?\" The \"linear\" in the term linear regression refers not to the data, but to the underlying model we are attempting to fit for. Let's use standard polynomials as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In general, a standard polynomial can be written as\n",
    "\n",
    "\\begin{align}\n",
    "y = a_0\\cdot x^n + a_1\\cdot x^{n-1} + \\ldots + a_{n-1}\\cdot x^1 + a_n\\cdot x^0\n",
    "\\end{align}\n",
    "\n",
    "where $y$ is the dependent variable, $x$ is the independent variable and $a_n$ are the coefficients of each term in the polynomial. Note that the model function, $y$, is *linear with respect to the coefficients, $a$.* In other words, we don't have terms like $a_n^2$, or $\\sin(a_n)$ in the equation. This means we can therefore write it as\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{y} = \\boldsymbol{A}\\boldsymbol{x}\n",
    "\\end{align}\n",
    "\n",
    "where $\\boldsymbol{y} = [y],\\ \\boldsymbol{A} = [x^n, x^{n-1}, \\ldots, x^1, x^0],$ and $\\boldsymbol{x} = [a_0, a_1, \\ldots, a_n]$, and $\\boldsymbol{A}\\boldsymbol{x}$ denotes a *dot product*. Note that $\\boldsymbol{x}$ in the above equation **is not** $x$ in the other equation, but is a vector containing the polynomial weights. Any set of equations we can decompose like this is considered a linear model. **The goal of linear regression is to solve for the weights of our model, $\\boldsymbol{x}$.**\n",
    "\n",
    "Once we've chosen our model, say a 1st order polynomial (which is a line), how can we use linear regression to find the best fit (i.e., to regress for the weight vector)? We can do this with a method known as **Linear Least Squares** (LLS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear Least Squares\n",
    "\n",
    "In the linear least squares method, we construct what's called an *objective function*, which is something that is a function of our model weights and our data, $f(\\boldsymbol{x}\\ |\\ \\boldsymbol{y})$. The choice of $\\boldsymbol{x}$ that minimizes $f$ gives us our best-fit. The simplest objective function is the sum of the squared distances of the model predictions from the data, which is to say\n",
    "\n",
    "\\begin{align}\n",
    "f = \\sum_i \\left(y_i - \\sum_jA_{ij}\\cdot x_j\\right)^2\n",
    "\\end{align}\n",
    "\n",
    "This is demonstrated in the figure below.\n",
    "\n",
    "<img src='imgs/lls.png' width=400px/>\n",
    "\n",
    "Let's use this figure as a quantitative example. We have four data points, with $(x,y) = [(1,6),\\ (2,5),\\ (3,7),\\ (4,10)]$. Let's say we want to fit a line to this data in the form\n",
    "\n",
    "\\begin{align}\n",
    "y = a_0x + a_1\n",
    "\\end{align}\n",
    "\n",
    "Given our model, we can write its predictions for the data as a system of equations, given as\n",
    "\n",
    "\\begin{align}\n",
    "\\left[\\begin{array}{c}\n",
    "y_1\\\\ y_2\\\\ y_3\\\\ y_4\\end{array}\\right]\n",
    "= \\left[\\begin{array}{cc}\n",
    "a_0x_1 + a_1\\\\ a_0x_2 + a_1\\\\ a_0x_3 + a_1\\\\ a_0x_4 + a_1\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We can further represent this in the matrix form we saw before as\n",
    "\n",
    "\\begin{align}\n",
    "\\left[\\begin{array}{c}\n",
    "y_1\\\\ y_2\\\\ y_3\\\\ y_4\\end{array}\\right]\n",
    "&= \\left[\\begin{array}{cc}\n",
    "x_1\\ 1\\\\ x_2\\ 1\\\\ x_3\\ 1\\\\ x_4\\ 1\\end{array}\\right]\\cdot\n",
    "\\left[\\begin{array}{c}\n",
    "a_0\\\\ a_1\\end{array}\\right]\\\\\n",
    "\\\\\n",
    "\\boldsymbol{y} &= \\boldsymbol{A}\\boldsymbol{x}\n",
    "\\end{align}\n",
    "\n",
    "The question we want to answer is, what choice of $a_0$ and $a_1$ give us the best-fit line? Well, let's just take a wild guess (not looking at the figure) and say $a_0 = 2$ and $a_1 = 1$. How good is this fit? To answer that question, let's evaluate our objective function, $f$. The data, $y_n$, are just the y-values of the data: $y = [6, 5, 7, 10]$. Our model prediction is found by evaluating $2\\cdot x + 1$ for $x = [1,2,3,4]$. This means our objective function is:\n",
    "\n",
    "\\begin{align}\n",
    "f &= \\left[(6 - (2*1 + 1))^2 + (5 - (2*2+1))^2 + (7 - (2*3 + 1))^2 + (10 - (2*4 + 1))^2\\right]\\\\\n",
    "  &= 10\n",
    "\\end{align}\n",
    "\n",
    "Is this a good answer? We don't know. Furthermore, we don't particularly care about any *individual* output of the objective function. What we care about is *minimizing the function*. Now let's try a better guess and see how it changes. Let's try $a_0 = \\tfrac{4}{3}$ and $a_1 = 4$. Our our objective function is then\n",
    "\n",
    "\\begin{align}\n",
    "f &= \\left[(6 - (\\tfrac{4}{3}*1 + 4))^2 + (5 - (\\tfrac{4}{3}*2+4))^2 + (7 - (\\tfrac{4}{3}*3 + 4))^2 + (10 - (\\tfrac{4}{3}*4 + 4))^2\\right]\\\\\n",
    "  &= 4.67\n",
    "\\end{align}\n",
    "\n",
    "This is a better result, meaning our second guess is a better fit to the data than our first guess! How can we do this automatically, without having to guess and check? To do that we need to minimize $f$ with respect to $x$, which we can do by taking its derivative and setting it equal to zero.\n",
    "\n",
    "\n",
    "How can we minimize a function with respect to its inputs? To do that, we can take its derivative and set it equal to zero.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x_k} = \\sum_i 2\\left(y_i-\\sum_j A_{ij}x_j\\right)(-A_{ik}) = 0\n",
    "\\end{align}\n",
    "\n",
    "If we re-arrange the terms above, we get the normal equations:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_i\\sum_j A_{ik} A_{ij}\\hat{x}_j = \\sum_i X_{ij}y_i\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat{x}$ is the value of $x$ that minimizes $f$. This can be rewritten in matrix notation as\n",
    "\n",
    "\\begin{align}\n",
    "(\\boldsymbol{A}^T\\boldsymbol{A})\\hat{\\boldsymbol{x}} = \\boldsymbol{A}^T\\boldsymbol{y}\n",
    "\\end{align}\n",
    "\n",
    "To solve this for $\\hat{\\boldsymbol{x}}$, we can take the inverse of the term on the left and multiply both sides to get:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\boldsymbol{x}} = \\left(\\boldsymbol{A}^T\\boldsymbol{A}\\right)^{-1}\\boldsymbol{A}^T\\boldsymbol{y}\n",
    "\\end{align}\n",
    "\n",
    "This is the equation we can use in order to find the best-fit $x$ for us. Let's try it in the former case of fitting a line to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up data vector and design matrix\n",
    "y = np.array([6.0,5.0,7.0,10.0])\n",
    "A = np.array([[1.0, 1.0],\n",
    "              [2.0, 1.0],\n",
    "              [3.0, 1.0],\n",
    "              [4.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.4  3.5]\n"
     ]
    }
   ],
   "source": [
    "# find x-hat!\n",
    "xhat = np.dot(np.dot(la.inv(np.dot(A.T, A)), A.T), y)\n",
    "print(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1141ae7f0>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAAG1CAYAAABqP/Q8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1141a6fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graph best-fit\n",
    "N = 100\n",
    "xdata = np.array([1.0,2.0,3.0,4.0])\n",
    "yhat = np.dot(A, xhat)\n",
    "\n",
    "# figure\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.grid(True)\n",
    "\n",
    "# axes\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('y', fontsize=18)\n",
    "\n",
    "# plot\n",
    "ax.plot(xdata, y, linestyle='', marker='o', color='steelblue', markersize=10)\n",
    "ax.plot(xdata, yhat, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 1: Fit a line to data\n",
    "\n",
    "Load the data from `data/noisydata.txt` and fit a line to it using LLS. Plot the data and your best-fit line, is it a good fit?\n",
    "\n",
    "Hint: An easy way to generate a design matrix for a polynomial is to do something like this:\n",
    "```\n",
    "np.vstack([xdata, np.ones_like(xdata)]).T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: Try fitting a quadratic to the data. Is it a better fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Least Squares\n",
    "\n",
    "We've looked at line-fitting for data that is relatively \"well-behaved,\" in that it has low-noise and no apparent outlier populations. For data that is noisy, or that has some data points that have large error, we can create a new objective function to account for the measurement error on our data. We do this by *dividing by the expected measurement error*:\n",
    "\n",
    "\\begin{align}\n",
    "f = \\sum_i \\left(\\frac{y_i - \\sum_jA_{ij}\\cdot x_j}{\\sigma_i}\\right)^2,\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma_i$ is the measurement error on the $i$th data point. This is a way of enacting a weighting on how the data influences the best-fit. One can show through similar steps from before that our final best-fit coefficient vector can now be calculated as\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\boldsymbol{x}} = \\left(\\boldsymbol{A}^T\\boldsymbol{N}^{-1}\\boldsymbol{A}\\right)^{-1}\\boldsymbol{A}^T\\boldsymbol{N}^{-1}\\boldsymbol{y},\n",
    "\\end{align}\n",
    "\n",
    "where the matrix $\\boldsymbol{N}$ contains the variance of the error along its diagonal: \n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{N} = \\left[\\begin{array}{cccc}\n",
    "\\sigma_0^2 & 0 & \\ldots & 0\\\\\n",
    "0 & \\sigma_1^2 & \\ldots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\ldots & \\sigma_n^2\\end{array}\\right]\n",
    "\\end{align}\n",
    "\n",
    "For data where the measurement error is the same for all data points (aka *homoscedastic*), then our weighting does not change the result. For data where certain points have larger measurement error than others (aka *heteroscedastic*), then the weighted LS will produce different results than the unweighted LS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Breakout 2: Use weighted LS to fit noisy data\n",
    "\n",
    "Load-in the data from `data/heteroscedastic_data.txt`.\n",
    "\n",
    "1.\n",
    "Make a plot of the data (with errorbars), and the weighted LS best-fit and unweighted LS best-fit of a quadratic model.\n",
    "\n",
    "Hint: To generate a diagonal matrix $\\boldsymbol{N}$ given a vector of the measurement errors, `epsilon`$ = [\\sigma_0,\\ \\sigma_1,\\ \\ldots,\\ \\sigma_n]$, use the following syntax:\n",
    "```\n",
    "N = np.eye(len(epsilon)) * epsilon**2\n",
    "```\n",
    "\n",
    "Don't forget you can plot data with measurement errors with the following syntax\n",
    "```\n",
    "ax.errorbar(xdata, ydata, yerr=yerrors, fmt='o')\n",
    "```\n",
    "\n",
    "2.\n",
    "Why do you think the unweighted LS best-fit and weighted LS best-fit differ in the way they do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Application: Galaxy Property Correlations\n",
    "\n",
    "Load in the `data/galaxydata.txt` file. It has three columns, $\\ln[\\text{Mass}]$, $\\ln[\\text{SFR}]$ and $12 + \\ln[O/H]$. We call these the galaxies mass, star-formation rate and metallicity respectively. This is data on ten-thousand nearby galaxies taken by the Sloan Digital Sky Survey optical survey.\n",
    "\n",
    "1.\n",
    "On one figure, make three subplots: metallicity vs. mass, metallicity vs. SFR, and SFR vs. mass. \n",
    "\n",
    "You should see a correlation between all the variables in the data. The question we'd like to ask ourselves, is which variable **drives** the correlation. That is to say, are SFR and metallicity intrinsically correlated? Or are they both just correlated to mass?\n",
    "\n",
    "To answer this, fit a line to the SFR vs. Mass relation using linear least squares. Call this line our \"expected SFR\". Use this line to subtract the expected SFR for each galaxy given its mass from its true SFR and call this the SFR residual:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{SFR}_{resid} = \\text{SFR}_{true} - \\text{SFR}_{synth}\n",
    "\\end{align}\n",
    "\n",
    "Then make a plot of metallicity vs. $\\text{SFR}_{resid}$. Is a correlation between metallicity and SFR present after subtracting out the SFR vs. mass correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
