{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "**Nick Kern**\n",
    "<br>\n",
    "**Astro 9: Python Programming in Astronomy**\n",
    "<br>\n",
    "**UC Berkeley**\n",
    "\n",
    "Reading: [Chp. 5, Computational Physics w/ Python](http://www-personal.umich.edu/~mejn/computational-physics/)\n",
    "\n",
    "Having learned techniques for numerical integration, now its time to learn techniques for numerical differentiation. We wil cover multiple ways to take a derivative, study their error properties, and apply them to an example problem.\n",
    "\n",
    "Recall the standard definition of a derivative as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{df}{dx} = \\lim_{h\\rightarrow0}\\frac{f(x+h) - f(x)}{h},\n",
    "\\end{align}\n",
    "\n",
    "where we can't physically make $h\\rightarrow0$, but we can make it very small to get a good approximation. We will look at this and other definitions of the derivative and their practical implementations. We call these kinds of derivative approximation formulas **finite difference** formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite Differences\n",
    "\n",
    "<img src='imgs/finite_diff.png' width=400px/>\n",
    "<center>Forward and backward finite difference. IC: Newman, Fig. 5.9</center>\n",
    "\n",
    "A **forward finite difference** is exactly the standard definition of the derivative. Assuming we are starting at the middle point in the above, figure, we calculate $f(x+h)$ and use the standard definition to get $f^{\\prime}(x)$. The **backward finite difference**, as suggested by the figure, can then be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{df}{dx} = \\lim_{h\\rightarrow0}\\frac{f(x)-f(x-h)}{h}\n",
    "\\end{align}\n",
    "\n",
    "There is typically no reason to prefer one over the other, and mostly one would use a forward difference. One reason to prefer one over the other, however, might be if one is approaching a discontinuity in $f(x)$, or if $f(x)$ is bounded and we are one edge of it.\n",
    "\n",
    "You might have noticed that the forward and backward derivative doesn't really look like a good approximation of the true derivative at $x$, in fact, it looks like a better approximation at $x+\\frac{1}{2}h$, or halfway between the two points. We can, in fact, create a better finite difference method using this concept, which is called a **central finite difference**. As you may have guessed, it is written as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{df}{dx} &= \\lim_{h\\rightarrow0}\\frac{f(x+h/2) - f(x-h/2)}{h}\\\\\n",
    "&\\text{or}\\\\\n",
    "\\frac{df}{dx} &= \\lim_{h\\rightarrow0}\\frac{f(x+h) - f(x-h)}{2h}\n",
    "\\end{align}\n",
    "\n",
    "Note that for a central difference of $f$ at $x$, we don't actually need to know the value of $f(x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 1\n",
    "\n",
    "1.\n",
    "Let's write our own forward, backward and central finite difference functions. This should be quick, because their are each two-liners and I've prepared starter code for you.\n",
    "\n",
    "2.\n",
    "Take the forward, backward and central derivatives of $f(x) = x^{3} - 4x^{2} + 2x$ at $x = 1$. Compare to the true answer derived by-hand. Which method is the most accurate? How does this accuracy depend on your selection of $h$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x**3 - 4*x**2 + 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_diff(func, x, h=1e-4):\n",
    "\n",
    "    return (func(x + h) - func(x)) / h\n",
    "    \n",
    "def backward_diff(func, x, h=1e-4):\n",
    "    \n",
    "    return (func(x) - func(x-h)) / h\n",
    "\n",
    "def central_diff(func, x, h=1e-4):\n",
    "    \n",
    "    return (func(x+h/2) - func(x-h/2)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_ans = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward diff yields error of 0.0002667017\n",
      "backward diff yields error of 0.0000446571\n",
      "central diff yields error of 0.0002667017\n"
     ]
    }
   ],
   "source": [
    "# print out accuracy of each method, what happens when h gets too small?\n",
    "h = 1e-12\n",
    "print(\"forward diff yields error of {:.10f}\".format(true_ans - forward_diff(func, 1, h)) )\n",
    "print(\"backward diff yields error of {:.10f}\".format(true_ans - backward_diff(func, 1, h )) )\n",
    "print(\"central diff yields error of {:.10f}\".format(true_ans - central_diff(func, 1, h )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "As with our numerical integrals, a numerical derivative is not exact and is subject to numerical errors, particularly the **rounding error** and **subtraction error** we discussed before, but another is one that is more obvious from the figure above, which is **approximation error**. Let's discuss this in the context of a Taylor expansion. \n",
    "\n",
    "Recall that **any** function can be approximated at a given $x_{0}$ using a [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series), which has an infinite number of terms. Each term in the sum relies on a higher order derivative of $f(x)$, and pushes the accuracy of the expansion to larger values of $|x - x_{0}|$. In general, a taylor expansion is written as\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = f(x_{0}) + \\frac{(x-x_{0})}{1!}f^{\\prime}(x_{0}) + \\frac{(x-x_{0})^{2}}{2!}f^{\\prime\\prime}(x_{0}) + \\frac{(x-x_{0})^{3}}{3!}f^{\\prime\\prime\\prime}(x_{0}) + \\ldots,\n",
    "\\end{align}\n",
    "where $f^{\\prime}$ represents the first derivative, $f^{\\prime\\prime}$ the second derivative and so on.\n",
    "\n",
    "In our context, we are seeking to use the derivative of $f$ at $x$ to recover the function value at $x+h$, meaning  we can express the Taylor expansion about $x$ as\n",
    "\n",
    "\\begin{align}\n",
    "f(x+h) = f(x) + h\\cdot f^{\\prime}(x) + \\frac{1}{2}h^{2}\\cdot f^{\\prime\\prime}(x) + \\ldots\n",
    "\\end{align}\n",
    "\n",
    "**Forward and Backward Difference**\n",
    "\n",
    "If we rearrange this expression for $f^{\\prime}(x)$, we find\n",
    "\n",
    "\\begin{align}\n",
    "f^{\\prime}(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{1}{2}h\\cdot f^{\\prime\\prime}(x) + \\ldots,\n",
    "\\end{align}\n",
    "\n",
    "which looks exactly like our forward difference formula plus a leading order error term the scales as $\\mathcal{O}(h)$. This means that, to leading order, the error on a forward / backward difference scales as $h$ and has an approximate magnitude of $\\frac{1}{2}h\\left|f^{\\prime\\prime}\\right|$.\n",
    "\n",
    "**Central Difference**\n",
    "\n",
    "What about for a central difference? Here, we need to write two Taylor expansions:\n",
    "\n",
    "\\begin{align}\n",
    "f(x+h) &= f(x) + h\\cdot f^{\\prime}(x) + \\frac{1}{2}h^{2}\\cdot f^{\\prime\\prime}(x) + \\frac{1}{9}h^{3}\\cdot f^{\\prime\\prime\\prime}(x) + \\ldots \\\\\n",
    "\\\\\n",
    "f(x-h) &= f(x) - h\\cdot f^{\\prime}(x) + \\frac{1}{2}h^{2}\\cdot f^{\\prime\\prime}(x) - \\frac{1}{9}h^{3}\\cdot f^{\\prime\\prime\\prime}(x) + \\ldots\n",
    "\\end{align}\n",
    "\n",
    "and if we subtract the second from the first, we are left with\n",
    "\n",
    "\\begin{align}\n",
    "f^{\\prime}(x) = \\frac{f(x+h) - f(x-h)}{2h} - \\frac{1}{9}h^{2}f^{\\prime\\prime\\prime}(x) + \\ldots\n",
    "\\end{align}\n",
    "\n",
    "The first term is exactly our central difference formula, meaning the central difference has an error that goes as $\\mathcal{O}(h^{2})$ to leading order. This confirms our intuition from before, showing that the error on a central difference scales as $h^{2}$ rather than just $h$ of the forward difference.\n",
    "\n",
    "**Ideal Step Size $h$**\n",
    "\n",
    "We can't make $h$ arbitrarily small; however, we saw before that a too-small $h$ actually makes the error diverge due to subtraction error. So, what's the best value of $h$? What's the sweet spot that minimizes both the subtraction error and the approximation error? You can find a quantitative argument in Newman 5.9.2, which says that\n",
    "\n",
    "\\begin{align}\n",
    "h_{\\text{ideal}} = \\sqrt{4C\\left|\\frac{f(x)}{f^{\\prime\\prime}(x)}\\right|}\n",
    "\\end{align}\n",
    "\n",
    "for a forward or backward difference, where $C$ is the machine floating-point precision of $10^{-16}$. If $f(x) \\sim f^{\\prime\\prime}(x) \\sim 1$, then $\\boxed{h_{\\text{ideal}} = 10^{-8}}$ for a forward or backward difference. For a central difference, we get\n",
    "\n",
    "\\begin{align}\n",
    "h_{\\text{ideal}} = \\left(24C\\left|\\frac{f(x)}{f^{\\prime\\prime\\prime}(x)}\\right|\\right)^{1/3},\n",
    "\\end{align}\n",
    "meaning, assuming $f(x) \\sim f^{\\prime\\prime\\prime}(x) \\sim 1$, then $\\boxed{h_{\\text{ideal}} = 10^{-5}}$ for a central difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 2\n",
    "\n",
    "Let's confirm graphically the optimal step size, $h$, for a forward and central difference.\n",
    "\n",
    "1.\n",
    "Differentiate the same function as before, $f(x) = x^{3} - 4x^{2} + 2x$ at $x=1$ with a range of $h$ values, from $10^{-12} < h < 10^{-2}$, and store the true answer minus the numerical approximation into an array called `epsilon`, which we will take to be the error, $\\epsilon$. Do this for all three derivative methods. Foreshadowing: we will want to plot our results in log-log space. In order to draw samples of $h$ evenly in log-space, you can use the `numpy.logspace()` function instead of the `linspace()` function.\n",
    "\n",
    "2.\n",
    "Make a plot of $\\epsilon$ on the y-axis and $h$ on the x-axis in log-log space. What is the optimal step-size for taking a derivative of this function at $x=1$ for each of the three methods? What is the smallest error you can achieve?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Derivatives\n",
    "\n",
    "The second derivative is the derivative of the first derivative. We can find this numerically by applying the finite difference formulat twice in a row. Using the central difference formula, we find\n",
    "\n",
    "\\begin{align}\n",
    "f^{\\prime\\prime}(x) &\\simeq \\frac{f^{\\prime}(x+h/2) - f^{\\prime}(x-h/2)}{h}\\\\\n",
    "\\\\\n",
    "&= \\frac{\\left[f(x+h) - f(x)\\right]/h - \\left[f(x)-f(x-h)\\right]/h}{h}\\\\\n",
    "\\\\\n",
    "&= \\frac{f(x+h) - 2f(x) + f(x-h)}{h^{2}}.\n",
    "\\end{align}\n",
    "\n",
    "Note that while we only need two points to calculate a first derivative, we now need three points to calculate a second derivative.\n",
    "\n",
    "Going through similar error arguments as before, we can find that the error of a central difference second derivative scales as $\\mathcal{O}(h^{2})$, similar to its first derivative counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 3\n",
    "\n",
    "1.\n",
    "Write a function for a central-difference second derivative (again a two-liner). Use it to calculate the second derivative of $f(x) = x^{3} - 4x^{2} + 2x$ at $x=1$ and compare to your expectation from analytics.\n",
    "\n",
    "2.\n",
    "Write a new central difference second derivative function that can be used on sampled data, rather than a continuous function. Look at the data in `freefall.txt`, which contains position and time information for an object in free fall near the Earth's surface. Give $N$ evenly-spaced time samples in `freefall.txt`, how many samples of the second-derivative will you have using the central-difference method? Use your new second derivative function to calculate the Earth's gravitational acceleration in $\\text{meters}\\ \\text{sec}^{-2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives\n",
    "\n",
    "Partial derivatives are derivatives of functions with more than one independent variable (multivariate) with respect to only one of those variables. First and second partial derivatives are the same as their univariate derivative counterparts, with the implication that one holds all other independent variables constant while doing the finite difference. For example, the central difference for the first partial derivative with respect to both $x$ and $y$ can be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x} &= \\frac{f(x+h/2, y) - f(x-h/2, y)}{h}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial f}{\\partial y} &= \\frac{f(x, y+h/2) - f(x, y-h/2)}{h}\n",
    "\\end{align}\n",
    "\n",
    "The second partial derivative with respect to only $x$ and only $y$ can be found similarly as before and yields the same equation. The second partial derivative with respect to both $x$ and $y$ can be found using the same second-derivative method as before, but the answer is quantitatively different, and can be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial^{2}f}{\\partial x\\partial y} = \\frac{f(x+h/2, y+h/2) - f(x-h/2, y+h/2) - f(x+h/y, y-h/2) + f(x-h/2, y-h/2)}{h^{2}}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 4: Calculate Force Given Potential\n",
    "\n",
    "The negative spatial gradient of the potential yields the force vector:\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{F}(x, y) = -\\nabla U(x, y) = \\left<\\frac{\\partial U}{\\partial x}, \\frac{\\partial U}{\\partial y}\\right>,\n",
    "\\end{align}\n",
    "\n",
    "where $\\frac{\\partial}{\\partial x}$ represents a partial derivative.\n",
    "\n",
    "1.\n",
    "Given the Rosenbrock function below, calculate the force vector on a 2D grid of points from $-1 < x < 1$ and $-1 < y < 1$.\n",
    "\n",
    "2.\n",
    "Make a **density plot of the magnitude** of the force vector using `imshow()`, and on the same plot, make a **plot the vector field** with the [`quiver()`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.axes.Axes.quiver.html) function in `matplotlib`.\n",
    "\n",
    "Recall you can create a 2D grid of $X$ and $Y$ points like\n",
    "```\n",
    "X, Y = np.meshgrid(x_array, y_array)\n",
    "```\n",
    "where `x_array` and `y_array` the the steps in $x$ and $y$ (using something like `linspace` or `arange`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rosenbrock(x, y, a=1, b=100):\n",
    "    return (a-x)**2 +b*(y - x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
