{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Ordinary Differential Equations Numerically\n",
    "\n",
    "**Nick Kern**\n",
    "<br>\n",
    "**Astro 9: Python Programming in Astronomy**\n",
    "<br>\n",
    "**UC Berkeley**\n",
    "\n",
    "Reading: Chp. 8, Computational Physics w/ Python, Newman\n",
    "\n",
    "In this lecture, we are going to be talking about how to apply the concepts we've been building with numerical integration and differentiation to solving *ordinary differential equations*. A differential equation is just an equation that relates two variables together via a derivative. For example, one of the simplest DE is the equation of motion of an object with a constant acceleration:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} = at + v_{0}\n",
    "\\end{align}\n",
    "\n",
    "which can easily be solved by-hand using separation of variables and integrating to get $x(t)$.\n",
    "\n",
    "The general form of a first-order, ordinary differential equation looks like\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} = f(x, t),\n",
    "\\end{align}\n",
    "\n",
    "in which case, the independent variable is $t$ and the dependent variable is $x$. You may have noticed if you did the first integral in your head that in order to actually solve these equations, we need to not only integrate it but also provide a boundary condition or an initial condition (the integration constant!).\n",
    "\n",
    "Why do we care to solve an ODE numerically? In the case of our first example, we don't, really, because we can do that simply by-hand. However, there are *plenty* of equations in Astrophysics that either cannot be solved analytically, are just too tedious to be solved by-hand, or are neither but need to just be evaluated many, many times, in which case, we'd like a computer to do it for us!\n",
    "\n",
    "One good example of an applicable (second-order) differential equation is one that describes the motion of an object subject to a Newtonian gravitational force:\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{F} = ma = m\\frac{d^{2}x}{dt^{2}} = -\\frac{GMm}{r^{2}}{\\hat{r}}\n",
    "\\end{align}\n",
    "\n",
    "This form of equation applies not only to something like a planet orbiting a star, but also, for example, to a sattelite orbiting the Earth or to molecular dynamics. Solutions to equations of this form are generally applicable outside of just gravitational physics, and are particularly useful for determining the equation of motion of charged objects subject to an electric field because the electric force follows the same functional form.\n",
    "\n",
    "Let's first start by discussing ways we can solve first-order ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Euler's Method\n",
    "\n",
    "Suppose we are given an equation of the form $\\frac{dx}{dt} = f(x, t)$ and are given an initial condition of $x$ given some time $t_{1}$. **Our goal is to solve for the function $x(t)$** for all times $t$.\n",
    "\n",
    "Using a Taylor expansion, we can write the value of $x$ at some later time as:\n",
    "\n",
    "\\begin{align}\n",
    "x(t + \\Delta t) &= x(t) + \\Delta t\\frac{dx}{dt} + \\tfrac{1}{2}{\\Delta t}^{2}\\frac{d^{2}x}{dt^{2}} + \\ldots \\\\\n",
    "\\\\\n",
    "&= x(t) + \\Delta t\\cdot f(x, t) + \\mathcal{O}(\\Delta t^{2})\n",
    "\\end{align}\n",
    "\n",
    "If we take $\\Delta t$ to be small, than the first two terms of this equation may be a good approximation to the true answer:\n",
    "\n",
    "\\begin{align}\n",
    "x(t + \\Delta t) \\simeq x(t) + \\Delta t\\cdot f(x, t)\n",
    "\\end{align}\n",
    "\n",
    "Note that what we have above is nothing more than just an extrapolation of a function, $x(t)$, based on its first derivative, which we call $f(x, t) = \\frac{dx}{dt}$.\n",
    "\n",
    "In order to solve for $x$ at some later time $t_{2}$, we need only divide up our timeline from $t_{1} < t < t_{2}$ into a bunch of small chunks and use the above method *at each chunk* to reach our answer of $x(t_{2})$. This is called *Euler's Method*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Example**\n",
    "\n",
    "Let's use Euler's method to solve the differential equation\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} = -x^{3} + \\sin(t)\n",
    "\\end{align}\n",
    "\n",
    "with an initial condition of $x = 0$ when $t=0$. Find the value of $x(t = 10)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define first derivative function\n",
    "def f(x, t):\n",
    "    return -x**3 + np.sin(t)\n",
    "    \n",
    "\n",
    "# Define bounds and step-sizes\n",
    "t1 = 0\n",
    "t2 = 10\n",
    "N = 100\n",
    "dt = float(t2-t1)/N\n",
    "\n",
    "# Make t-points\n",
    "tpoints = np.linspace(t1, t2, N+1)\n",
    "\n",
    "# Define initial condition\n",
    "x = 0\n",
    "\n",
    "# Iterate Euler's Method to get x(t)\n",
    "xpoints = []\n",
    "for t in tpoints:\n",
    "    # append value of x to xpoints\n",
    "    xpoints.append(x)\n",
    "    \n",
    "    # update value using Euler's method\n",
    "    x += dt * f(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "# axes\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.grid(True)\n",
    "ax.set_xlim(-1,11)\n",
    "ax.set_ylim(-1.3,1.3)\n",
    "ax.set_xlabel('t', fontsize=18)\n",
    "ax.set_ylabel('x(t)', fontsize=18)\n",
    "\n",
    "# plot\n",
    "ax.plot(tpoints, xpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see from our Taylor expansion argument that the error term in Euler's method scales as $\\mathcal{O}(\\Delta t^{2})$. This is the error induced *at each point* in our iteration. You can see that each point relies on the answer from the previous point, meaning error in compounded all the way to our final answer, $x(t_{2}$). To get the error on our final answer, we need to sum the error term of each point:\n",
    "\n",
    "\\begin{align}\n",
    "\\epsilon_{t_{2}} &= \\sum_{k=0}^{N-1}\\tfrac{1}{2}{\\Delta t}^{2}\\left(\\frac{d^{2}x}{dt^{2}}\\right)_{t=t_{k}} = \\tfrac{1}{2}\\Delta t\\sum\\Delta t\\left(\\frac{df}{dt}\\right)\\\\\n",
    "\\\\\n",
    "&\\simeq \\tfrac{1}{2}\\Delta t\\int_{t_{1}}^{t_{2}}\\frac{df}{dt}dt = \\boxed{\\tfrac{1}{2}\\Delta t\\left[f(x_{t_{2}},t_{2}) - f(x_{t_{1}}, t_{1})\\right]}\n",
    "\\end{align}\n",
    "\n",
    "Note that in this case, the error on the final answer $x(t_{2})$ only scales as $\\Delta t^{1}$, which is worse than the error we accumulate per-point from Euler's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Breakout 1\n",
    "\n",
    "1.\n",
    "Put your above script into a function called `ode_solve` that takes as input the derivative function $f$, the initial condition of $x_1$ and $t_1$, the stopping time $t_2$ and the number of time steps $N$ between $t_1$ and $t_2$. Have it return the two arrays `tpoints` and `xpoints`.\n",
    "\n",
    "2.\n",
    "Use your function to solve for $x(t)$ up to $t_2 = 10$ for initial conditions of\n",
    "\n",
    "* $x = 0,\\ t_1 = 0$\n",
    "* $x = 1,\\ t_1 = 0$\n",
    "* $x = -1,\\ t_1 = 0$\n",
    "* $x = 0,\\ t_1 = 2$\n",
    "\n",
    "and plot all of these curves on a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define first derivative function\n",
    "def f(x, t):\n",
    "    return -x**3 + np.sin(t)\n",
    "\n",
    "# define ode solver using Euler's Method\n",
    "def ode_solve(f, x1, t1, t2, N=100):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return tpoints, xpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second-Order Runge-Kutta\n",
    "\n",
    "Euler's method may work well for our simple toy-model problem over a modest range of times. However, for more complex problems the poor error properties of Euler's method will prove fatal. Let's see if we can devise a more accurate algorithm for solving an ODE.\n",
    "\n",
    "Runge-Kutta methods are a class of methods for solving ODE's. The first-order RK method is, for example, just Euler's method. Higher order methods, like second-order and fourth-order methods, are more accurate than first-order RK and are therefore of great importance.\n",
    "\n",
    "Let's think back to our discussion of numerical derivatives. We saw two basic methods for performing a numerical derivative: the forward difference and central difference. The forward difference is similar to what we are doing in Euler's method, however, we saw that the central difference was much more accurate in estimating the derivative. The idea behind second-order Runge-Kutta is to use a central difference for the derivative, rather than the forward difference.\n",
    "\n",
    "<img src='imgs/rk2.png' width=400px/>\n",
    "<center>Figure 8.2 of Newman</center>\n",
    "\n",
    "Mathematically, this means we invoke a Taylor expansion not around $t$, but around $t + \\Delta t/2$, meaning we can get the value at $x(t+\\Delta t)$ as\n",
    "\n",
    "\\begin{align}\n",
    "x(t+\\Delta t) = x(t + \\tfrac{1}{2}\\Delta t) + \\tfrac{1}{2}\\Delta t\\left(\\frac{dx}{dt}\\right)_{t + \\tfrac{1}{2}\\Delta t} + \\tfrac{1}{8}\\Delta t^{2}\\left(\\frac{d^{2}x}{dt^{2}}\\right)_{t + \\tfrac{1}{2}\\Delta t} + \\mathcal{O}(\\Delta t^{3}).\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we get can the value of $x(t)$ using our expansion centered at $t + \\Delta t/2$ as\n",
    "\n",
    "\\begin{align}\n",
    "x(t) = x(t + \\tfrac{1}{2}\\Delta t) - \\tfrac{1}{2}\\Delta t\\left(\\frac{dx}{dt}\\right)_{t + \\tfrac{1}{2}\\Delta t} + \\tfrac{1}{8}\\Delta t^{2}\\left(\\frac{d^{2}x}{dt^{2}}\\right)_{t + \\tfrac{1}{2}\\Delta t} + \\mathcal{O}(\\Delta t^{3}).\n",
    "\\end{align}\n",
    "\n",
    "If we take the difference of these two equations, we are left with\n",
    "\n",
    "\\begin{align}\n",
    "x(t+\\Delta t) = x(t) + \\Delta t\\cdot f(x,t+\\tfrac{1}{2}\\Delta t) + \\mathcal{O}(\\Delta t^{3})\n",
    "\\end{align}\n",
    "\n",
    "Notice two major things: (1) the first two terms is similar to our Euler method from before, except now the error term goes not as $\\Delta t^{2}$ but as $\\Delta t^{3}$ (which is why this is called \"second-order\" RK) and (2) the evaluation of our derivative function $f$ is no longer at $t$ but at $t + \\frac{1}{2}\\Delta t$, which we don't know! We can get around this by using our Euler's method from before to make the approximation $x(t + \\frac{1}{2}\\Delta t) = x(t) + \\frac{1}{2}\\Delta t\\cdot f(x,t)$, and then insert that result into $f$ to complete the second-order Runge-Kutta method.\n",
    "\n",
    "The full method can therefore be written as\n",
    "\n",
    "\\begin{align}\n",
    "k_1 &= \\Delta t\\cdot f(x, t)\\\\\n",
    "k_2 &= \\Delta t\\cdot f(x+\\tfrac{1}{2}k_1, t + \\tfrac{1}{2}\\Delta t)\\\\\n",
    "x(t + \\Delta t) &= x(t) + k_2\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's modify our `ode_solve` function from before to incorporate the second-order Runge-Kutta method (aka, RK2). It is almost identical, with the exception of a few extra lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define first derivative function\n",
    "def f(x, t):\n",
    "    return -x**3 + np.sin(t)\n",
    "\n",
    "# Define bounds and step-sizes\n",
    "t1 = 0.0\n",
    "t2 = 10.0\n",
    "N = 10\n",
    "dt = float(t2-t1)/N\n",
    "\n",
    "# Make t-points\n",
    "tpoints = np.linspace(t1, t2, N+1)\n",
    "\n",
    "# Define initial condition\n",
    "x = 0.0\n",
    "\n",
    "# Iterate Euler's Method to get x(t)\n",
    "xpoints = []\n",
    "for t in tpoints:\n",
    "    # append value of x to xpoints\n",
    "    xpoints.append(x)\n",
    "    \n",
    "    # update value using RK2 method\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth-Order Runge-Kutta\n",
    "\n",
    "We can continue to expand on our previous argument of taking central differences rather than forward differences to approximate the derivatives. The more sophisticated we get, however, the more complicated the equations become. A commonly agreed upon \"sweet-spot\" is the *fourth-order Runge-Kutta* method. We won't go through the derivation here, which is quite complicated and follows the same logic as RK2, but will quote the result for us to use. This fourth-order Runge-Kutta method (aka. RK4) has an error term that scales as $\\mathcal{O}(\\Delta t^{5})$, making it two orders better than RK2. The equations look like:\n",
    "\n",
    "\\begin{align}\n",
    "k_1 &= \\Delta t\\cdot f(x, t)\\\\\n",
    "k_2 &= \\Delta t\\cdot f(x + \\tfrac{1}{2}k_1, t + \\tfrac{1}{2}\\Delta t)\\\\\n",
    "k_3 &= \\Delta t\\cdot f(x + \\tfrac{1}{2}k_2, t + \\tfrac{1}{2}\\Delta t)\\\\\n",
    "k_4 &= \\Delta t\\cdot f(x + k_3, t + \\Delta t)\\\\\n",
    "\\\\\n",
    "x(t + \\Delta t) &= \\boxed{x(t) + \\tfrac{1}{6}(k_1 + 2k_2 + 3k_3 + k_4)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 2\n",
    "\n",
    "1.\n",
    "Take the above script for the second-order Runge-Kutta method and modify it to use the fourth-order RK method. Put it into a function called `RK4`, similar to your `ode_solve` function, which takes the same parameters.\n",
    "\n",
    "2.\n",
    "Use your `RK4` function to solve the same differential equation from before: $f(x, t) = \\frac{dx}{dt} = -x^{3} + \\sin(t)$ and make a plot of it. How many steps $N$ do we need to converge to a stable answer, as compared to what we got with Euler's method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RK4(f, x1, t1, t2, N=10):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return tpoints, xpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Equations with more than one Dependent Variable\n",
    "\n",
    "Up to now we have studied differential equations with one independent variable and one dependent variable. Often, however, problems we encounter have more than one dependent variable. We call these *simultaneous differential equations*. In this case, we have multiple dependent variables, but still only have a single independent variable. Take for example the following set of equations\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} = xy - x;\\ \\ \\frac{dy}{dt} = y - xy + \\sin^{2}(\\omega t),\n",
    "\\end{align}\n",
    "\n",
    "where $x$ and $y$ are the two dependent variables and $t$ is our single independent variable. We can generalize a set of simultaneous DE to the form\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} = f_{x}(x,y,t);\\ \\ \\frac{dy}{dt} = f_{y}(x,y,t)\n",
    "\\end{align}\n",
    "\n",
    "which can further be condensed using vector notation, $\\boldsymbol{r} = \\langle x, y\\rangle;\\ \\boldsymbol{f} = \\langle f_{x}, f_{y}\\rangle$, yielding\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\boldsymbol{r}}{dt} = \\boldsymbol{f}(\\boldsymbol{r}, t).\n",
    "\\end{align}\n",
    "\n",
    "Using this notation, simultaneous differential equations take the same form as before. Euler's method for simultaneous DE, for example, can be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{r}(t + \\Delta t) = \\boldsymbol{r}(t) + \\Delta t\\cdot\\boldsymbol{f}(\\boldsymbol{r},t),\n",
    "\\end{align}\n",
    "\n",
    "where $\\boldsymbol{r}$ could hold as many dependent variables as we like: $\\boldsymbol{r} = \\langle x, y, z, \\ldots\\rangle$.\n",
    "\n",
    "Similarly, our fourth-order Runge-Kutta equations can be written in vector form as\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{k}_1 &= \\Delta t\\cdot\\boldsymbol{f}(\\boldsymbol{r}, t)\\\\\n",
    "\\boldsymbol{k}_2 &= \\Delta t\\cdot\\boldsymbol{f}(\\boldsymbol{r}+\\tfrac{1}{2}\\boldsymbol{k}_1, t+\\tfrac{1}{2}\\Delta t)\\\\\n",
    "\\boldsymbol{k}_3 &= \\Delta t\\cdot\\boldsymbol{f}(\\boldsymbol{r}+\\tfrac{1}{2}\\boldsymbol{k}_1, t+\\tfrac{1}{2}\\Delta t)\\\\\n",
    "\\boldsymbol{k}_4 &= \\Delta t\\cdot\\boldsymbol{f}(\\boldsymbol{r}+\\tfrac{1}{2}\\boldsymbol{k}_1, t+\\tfrac{1}{2}\\Delta t)\\\\\n",
    "\\\\\n",
    "\\boldsymbol{r}(t + \\Delta t) &= \\boldsymbol{r}(t) + \\tfrac{1}{6}\\left(\\boldsymbol{k}_1 + 2\\boldsymbol{k}_2 + 2\\boldsymbol{k}_3 + \\boldsymbol{k}_4\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example **\n",
    "\n",
    "Let's apply our RK4 method in vector form to our simultaneous DE listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Write Function\n",
    "omega = 1.0\n",
    "def f(r, t):\n",
    "    # break into components\n",
    "    x = r[0]\n",
    "    y = r[1]\n",
    "    # get derivatives\n",
    "    fx = x*y - x\n",
    "    fy = y - x*y + np.sin(omega * t)**2\n",
    "    # return vector of derivatives\n",
    "    return np.array([fx, fy], dtype=np.float)\n",
    "\n",
    "# set bounds\n",
    "t1 = 0.0\n",
    "t2 = 10.0\n",
    "N = 100\n",
    "dt = float(t2 - t1) / N\n",
    "\n",
    "# Initialize points in time\n",
    "tpoints = np.linspace(t1, t2, N+1)\n",
    "\n",
    "# Initialize empty x and y lists\n",
    "xpoints = []\n",
    "ypoints = []\n",
    "\n",
    "# set initial condition\n",
    "r = np.array([1.0, 1.0])\n",
    "\n",
    "# loop over points in time\n",
    "for t in tpoints:\n",
    "    # append to x & y lists\n",
    "    xpoints.append( r[0] )\n",
    "    ypoints.append( r[1] )\n",
    "    \n",
    "    # calculate RK4\n",
    "    k1 = dt * f(r, t)\n",
    "    k2 = dt * f(r + 0.5*k1, t + 0.5*dt)\n",
    "    k3 = dt * f(r + 0.5*k2, t + 0.5*dt)\n",
    "    k4 = dt * f(r + k3, t + dt)\n",
    "    r += (k1 + 2*k2 + 2*k3 + k4) / 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "# ax1\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax1.grid(True)\n",
    "ax1.set_ylabel('x(t)', fontsize=18)\n",
    "ax1.plot(tpoints, xpoints, color='steelblue', linewidth=3)\n",
    "ax.set_ylim(0.8,2.5)\n",
    "\n",
    "# ax2\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "ax2.grid(True)\n",
    "ax2.set_xlabel('t', fontsize=20)\n",
    "ax2.set_ylabel('y(t)', fontsize=18)\n",
    "ax2.plot(tpoints, ypoints, color='darkorange', linewidth=3)\n",
    "ax2.set_ylim(0.3,1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a 3D plot using the techniques we learned in the maplotlib lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d plot\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "\n",
    "# axes\n",
    "ax.set_xlabel('t',fontsize=15)\n",
    "ax.set_ylabel('x',fontsize=15)\n",
    "ax.set_zlabel('y',fontsize=15)\n",
    "ax.set_xlim(0,10)\n",
    "ax.set_ylim(0.8, 2.5)\n",
    "ax.set_zlim(0.3, 1.8)\n",
    "\n",
    "# plot\n",
    "ax.scatter(tpoints, xpoints, ypoints, c='steelblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-Order Differential Equations\n",
    "\n",
    "Up to now we have studied differential equations that depend only on the first derivative with respect to the independent variable. However, we already know that some of the most important equations in physics--namely the relation between an object's motion and an acceleration induced by a force: $a(t) = \\frac{d^{2}x}{dt^{2}}$--involve second derivatives. Now that we have studied how to solve first-order DE, we can begin to look at how to solve second-order differential equations.\n",
    "\n",
    "The general form for a 2nd order DE, similar to our 1st order DE, can be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d^{2}x}{dt^{2}} = f(x, \\tfrac{dx}{dt}, t),\n",
    "\\end{align}\n",
    "\n",
    "which is to say that our 2nd order term can depend on the dependent and independent variables $x$ & $t$, as well as the 1st order differential $\\frac{dx}{dt}$. This could look like, for example, something along the lines of\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d^{2}x}{dt^{2}} = \\frac{1}{x}\\left(\\frac{dx}{dt}\\right)^{2} + 2\\frac{dx}{dt} - x^{3}e^{-4t}\n",
    "\\end{align}\n",
    "\n",
    "The trick to solving this equation is to define a new variable for our first derivative term, and then define our second order derivative as the first derivative of that variable. In other words:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} &= y\\\\\n",
    "\\frac{d^{2}x}{dt^{2}} &= \\frac{dy}{dt} = f(x,y,t)\n",
    "\\end{align}\n",
    "\n",
    "This has reduced a single second-order DE into two simultaneous first-order DEs, which we know how to solve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 3:\n",
    "\n",
    "The equation of motion for a point mass at the end of a massless and frictionless arm attached to a pivot can be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d^{2}\\theta}{dt^{2}} = -\\frac{g}{\\ell}\\sin\\theta\n",
    "\\end{align}\n",
    "where $\\theta$ is the angle displacement from center, $g$ is the gravitational acceleration and $\\ell$ is the length of the arm. In the limit that $\\theta << 1$ this can be linearized and solved exactly. In general, however, this is a nonlinear equation that cannot be solved exactly. We can, however, solve this on a computer by using the techniques we learned above.\n",
    "\n",
    "Start by creating a new variable `omega` such that\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\theta}{dt} &= \\omega\\\\\n",
    "&\\text{and}\\\\\n",
    "\\frac{d\\omega}{dt} &= -\\frac{g}{\\ell}\\sin\\theta.\n",
    "\\end{align}\n",
    "\n",
    "Then, use the technique we studied above for a simultaneous set of first-order differential equations to solve for the trajectory of $\\theta(t)$.\n",
    "\n",
    "You can assume $\\ell = 0.1$ meters. I have some code to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = 9.81\n",
    "l = 0.1\n",
    "\n",
    "def f(r, t):\n",
    "    theta = r[0]\n",
    "    omega = r[1]\n",
    "    ftheta = omega\n",
    "    fomega = -(g/l)*np.sin(theta)\n",
    "    return np.array([ftheta, fomega], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
