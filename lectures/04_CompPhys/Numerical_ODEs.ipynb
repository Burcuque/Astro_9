{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Ordinary Differential Equations Numerically\n",
    "\n",
    "**Nick Kern**\n",
    "<br>\n",
    "**Astro 9: Python Programming in Astronomy**\n",
    "<br>\n",
    "**UC Berkeley**\n",
    "\n",
    "Reading: Chp. 8, Computational Physics w/ Python, Newman\n",
    "\n",
    "In this lecture, we are going to be talking about how to apply the concepts we've been building with numerical integration and differentiation to solving *ordinary differential equations*. A differential equation is just an equation that relates two variables together via a derivative. For example, one of the simplest DE is the equation of motion of an object with a constant acceleration:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} = at + v_{0}\n",
    "\\end{align}\n",
    "\n",
    "which can easily be solved by-hand using separation of variables and integrating to get $x(t)$.\n",
    "\n",
    "The general form of a first-order, ordinary differential equation looks like\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} = f(x, t),\n",
    "\\end{align}\n",
    "\n",
    "in which case, the independent variable is $t$ and the dependent variable is $x$. You may have noticed if you did the first integral in your head that in order to actually solve these equations, we need to not only integrate it but also provide a boundary condition or an initial condition (the integration constant!).\n",
    "\n",
    "Why do we care to solve an ODE numerically? In the case of our first example, we don't, really, because we can do that simply by-hand. However, there are *plenty* of equations in Astrophysics that either cannot be solved analytically, are just too tedious to be solved by-hand, or are neither but need to just be evaluated many, many times, in which case, we'd like a computer to do it for us!\n",
    "\n",
    "One good example of an applicable (second-order) differential equation is one that describes the motion of an object subject to a Newtonian gravitational force:\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{F} = ma = m\\frac{d^{2}x}{dt^{2}} = -\\frac{GMm}{r^{2}}{\\hat{r}}\n",
    "\\end{align}\n",
    "\n",
    "This form of equation applies not only to something like a planet orbiting a star, but also, for example, to a sattelite orbiting the Earth or to molecular dynamics. Solutions to equations of this form are generally applicable outside of just gravitational physics, and are particularly useful for determining the equation of motion of charged objects subject to an electric field because the electric force follows the same functional form.\n",
    "\n",
    "Let's first start by discussing ways we can solve first-order ODEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Euler's Method\n",
    "\n",
    "Suppose we are given an equation of the form $\\frac{dx}{dt} = f(x, t)$ and are given an initial condition of $x$ given some time $t_{1}$. **Our goal is to solve for the function $x(t)$** for all times $t$.\n",
    "\n",
    "Using a Taylor expansion, we can write the value of $x$ at some later time as:\n",
    "\n",
    "\\begin{align}\n",
    "x(t + \\Delta t) &= x(t) + \\Delta t\\frac{dx}{dt} + \\frac{1}{2}{\\Delta t}^{2}\\frac{d^{2}x}{dt^{2}} + \\ldots \\\\\n",
    "\\\\\n",
    "&= x(t) + \\Delta t\\cdot f(x, t) + \\mathcal{O}(\\Delta t^{2})\n",
    "\\end{align}\n",
    "\n",
    "If we take $\\Delta t$ to be small, than the first two terms of this equation may be a good approximation to the true answer:\n",
    "\n",
    "\\begin{align}\n",
    "x(t + \\Delta t) \\simeq x(t) + \\Delta t\\cdot f(x, t)\n",
    "\\end{align}\n",
    "\n",
    "Note that what we have above is nothing more than just an extrapolation of a function, $x(t)$, based on its first derivative, which we call $f(x, t) = \\frac{dx}{dt}$.\n",
    "\n",
    "In order to solve for $x$ at some later time $t_{2}$, we need only divide up our timeline from $t_{1} < t < t_{2}$ into a bunch of small chunks and use the above method *at each chunk* to reach our answer of $x(t_{2})$. This is called *Euler's Method*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Example**\n",
    "\n",
    "Let's use Euler's method to solve the differential equation\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} = -x^{3} + \\sin(t)\n",
    "\\end{align}\n",
    "\n",
    "with an initial condition of $x = 0$ when $t=0$. Find the value of $x(t = 10)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define first derivative function\n",
    "\n",
    "\n",
    "# Define bounds and step-sizes\n",
    "t1 = \n",
    "t2 = \n",
    "N =\n",
    "dt =\n",
    "\n",
    "# Make t-points\n",
    "tpoints = \n",
    "\n",
    "# Define initial condition\n",
    "x =\n",
    "\n",
    "# Iterate Euler's Method to get x(t)\n",
    "xpoints = []\n",
    "for t in tpoints:\n",
    "    # append value of x to xpoints\n",
    "    \n",
    "    # update value using Euler's method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# figure\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "# axes\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.grid(True)\n",
    "ax.set_xlim(-1,11)\n",
    "ax.set_ylim(-1.3,1.3)\n",
    "ax.set_xlabel('t', fontsize=18)\n",
    "ax.set_ylabel('x(t)', fontsize=18)\n",
    "\n",
    "# plot\n",
    "ax.plot(tpoints, xpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see from our Taylor expansion argument that the error term in Euler's method scales as $\\mathcal{O}(\\Delta t^{2})$. This is the error induced *at each point* in our iteration. You can see that each point relies on the answer from the previous point, meaning error in compounded all the way to our final answer, $x(t_{2}$). To get the error on our final answer, we need to sum the error term of each point:\n",
    "\n",
    "\\begin{align}\n",
    "\\epsilon_{t_{2}} &= \\sum_{k=0}^{N-1}\\frac{1}{2}{\\Delta t}^{2}\\left(\\frac{d^{2}x}{dt^{2}}\\right)_{t=t_{k}} = \\frac{1}{2}\\Delta t\\sum\\Delta t\\left(\\frac{df}{dt}\\right)\\\\\n",
    "\\\\\n",
    "&\\simeq \\frac{1}{2}\\Delta t\\int_{t_{1}}^{t_{2}}\\frac{df}{dt}dt = \\boxed{\\frac{1}{2}\\Delta t\\left[f(x_{t_{2}},t_{2}) - f(x_{t_{1}}, t_{1})\\right]}\n",
    "\\end{align}\n",
    "\n",
    "Note that in this case, the error on the final answer $x(t_{2})$ only scales as $\\Delta t^{1}$, which is worse than the error we accumulate per-point from Euler's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Breakout 1\n",
    "\n",
    "1.\n",
    "Put your above script into a function called `ode_solve` that takes as input the derivative function $f$, the initial condition of $x_1$ and $t_1$, the stopping time $t_2$ and the number of time steps $N$ between $t_1$ and $t_2$. Have it return the two arrays `tpoints` and `xpoints`.\n",
    "\n",
    "2.\n",
    "Use your function to solve for $x(t)$ for initial conditions of\n",
    "\n",
    "* x = 0, t = 0\n",
    "* x = 1, t = 0\n",
    "* x = -1, t = 0\n",
    "* x = 0, t = 2\n",
    "\n",
    "and plot all of these curves on a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ode_solve(f, x1, t1, t2, N=100):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return tpoints, xpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second-Order Runge-Kutta\n",
    "\n",
    "Euler's method may work well for our simple toy-model problem over a modest range of times. However, for more complex problems the poor error properties of Euler's method will prove fatal. Let's see if we can devise a more accurate algorithm for solving an ODE.\n",
    "\n",
    "Runge-Kutta methods are a class of methods for solving ODE's. The first-order RK method is, for example, just Euler's method. Higher order methods, like second-order and fourth-order methods, are more accurate than first-order RK and are therefore of great importance.\n",
    "\n",
    "Let's think back to our discussion of numerical derivatives. We saw two basic methods for performing a numerical derivative: the forward difference and central difference. The forward difference is similar to what we are doing in Euler's method, however, we saw that the central difference was much more accurate in estimating the derivative. The idea behind second-order Runge-Kutta is to use a central difference for the derivative, rather than the forward difference.\n",
    "\n",
    "<img src='imgs/rk2.png' width=400px/>\n",
    "<center>Figure 8.2 of Newman</center>\n",
    "\n",
    "Mathematically, this means we invoke a Taylor expansion not around $t$, but around $t + \\Delta t/2$, meaning we can get the value at $x(t+\\Delta t)$ as\n",
    "\n",
    "\\begin{align}\n",
    "x(t+\\Delta t) = x(t + \\frac{1}{2}\\Delta t) + \\frac{1}{2}\\Delta t\\left(\\frac{dx}{dt}\\right)_{t + \\frac{1}{2}\\Delta t} + \\frac{1}{8}\\Delta t^{2}\\left(\\frac{d^{2}x}{dt^{2}}\\right)_{t + \\frac{1}{2}\\Delta t} + \\mathcal{O}(\\Delta t^{3}).\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we get can the value of $x(t)$ using our expansion centered at $t + \\Delta t/2$ as\n",
    "\n",
    "\\begin{align}\n",
    "x(t) = x(t + \\frac{1}{2}\\Delta t) - \\frac{1}{2}\\Delta t\\left(\\frac{dx}{dt}\\right)_{t + \\frac{1}{2}\\Delta t} + \\frac{1}{8}\\Delta t^{2}\\left(\\frac{d^{2}x}{dt^{2}}\\right)_{t + \\frac{1}{2}\\Delta t} + \\mathcal{O}(\\Delta t^{3}).\n",
    "\\end{align}\n",
    "\n",
    "If we take the difference of these two equations, we are left with\n",
    "\n",
    "\\begin{align}\n",
    "x(t+\\Delta t) = x(t) + \\Delta t\\cdot f(x,t+\\frac{1}{2}\\Delta t) + \\mathcal{O}(\\Delta t^{3})\n",
    "\\end{align}\n",
    "\n",
    "Notice two major things: (1) the first two terms is similar to our Euler method from before, except now the error term goes not as $\\Delta t^{2}$ but as $\\Delta t^{3}$ (which is why this is called \"second-order\" RK) and (2) the evaluation of our derivative function $f$ is no longer at $t$ but at $t + \\frac{1}{2}\\Delta t$, which we don't know! We can get around this by using our Euler's method from before to make the approximation $x(t + \\frac{1}{2}\\Delta t) = x(t) + \\frac{1}{2}\\Delta t\\cdot f(x,t)$, and then insert that result into $f$ to complete the second-order Runge-Kutta method.\n",
    "\n",
    "The full method can therefore be written as\n",
    "\n",
    "\\begin{align}\n",
    "k_1 &= \\Delta t\\cdot f(x, t)\\\\\n",
    "k_2 &= \\Delta t\\cdot f(x+\\frac{1}{2}k_1, t + \\frac{1}{2}\\Delta t)\\\\\n",
    "x(t + \\Delta t) &= x(t) + k_2\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's modify our `ode_solve` function from before to incorporate the second-order Runge-Kutta method (aka, RK2). It is almost identical, with the exception of a few extra lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define first derivative function\n",
    "def f(x, t):\n",
    "    return -x**3 + np.sin(t)\n",
    "\n",
    "# Define bounds and step-sizes\n",
    "t1 = 0.0\n",
    "t2 = 10.0\n",
    "N = 10\n",
    "dt = float(t2-t1)/N\n",
    "\n",
    "# Make t-points\n",
    "tpoints = np.linspace(t1, t2, N+1)\n",
    "\n",
    "# Define initial condition\n",
    "x = 0.0\n",
    "\n",
    "# Iterate Euler's Method to get x(t)\n",
    "xpoints = []\n",
    "for t in tpoints:\n",
    "    # append value of x to xpoints\n",
    "    xpoints.append(x)\n",
    "    \n",
    "    # update value using RK2 method\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth-Order Runge-Kutta\n",
    "\n",
    "We can continue to expand on our previous argument of taking central differences rather than forward differences to approximate the derivatives. The more sophisticated we get, however, the more complicated the equations become. A commonly agreed upon \"sweet-spot\" is the *fourth-order Runge-Kutta* method. We won't go through the derivation here, which is quite complicated, but will quote the result for us to use. This fourth-order Runge-Kutta method (aka. RK4) has an error term that scales as $\\mathcal{O}(\\Delta t^{5})$, making it two orders better than RK2. The equations look like:\n",
    "\n",
    "\\begin{align}\n",
    "k_1 &= \\Delta t\\cdot f(x, t)\\\\\n",
    "k_2 &= \\Delta t\\cdot f(x + \\frac{1}{2}k_1, t + \\frac{1}{2}\\Delta t)\\\\\n",
    "k_3 &= \\Delta t\\cdot f(x + \\frac{1}{2}k_2, t + \\frac{1}{2}\\Delta t)\\\\\n",
    "k_4 &= \\Delta t\\cdot f(x + k_3, t + \\Delta t)\\\\\n",
    "x(t + \\Delta t) &= \\boxed{x(t) + \\frac{1}{6}(k_1 + 2k_2 + 3k_3 + k_4)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout 2\n",
    "\n",
    "1.\n",
    "Take the above script for the second-order Runge-Kutta method and modify it to use the fourth-order RK method. Put it into a function called `RK4`, similar to your `ode_solve` function, which takes the same parameters.\n",
    "\n",
    "2.\n",
    "Use your `RK4` function to solve the same differential equation from before: $f(x, t) = \\frac{dx}{dt} = -x^{3} + \\sin(t)$ and make a plot of it. How many steps $N$ do we need to converge to a stable answer, as compared to what we got with Euler's method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RK4(f, x1, t1, t2, N=10):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return tpoints, xpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
